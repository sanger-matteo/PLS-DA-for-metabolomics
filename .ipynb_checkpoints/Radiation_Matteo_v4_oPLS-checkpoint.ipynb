{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pID_Survive:   201\n",
      "pID_Dead   :   49\n",
      "Smp_Survive:   798\n",
      "Smp_Dead   :   191\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ****************************************************************************\n",
    "#                       DATA ORGANIZATION and FORMATING\n",
    "# ****************************************************************************\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import nan\n",
    "\n",
    "# ------ LOAD data-tables ---------------------------------------------------- \n",
    "# List of clinical table with features and timepoints\n",
    "# => ClinicalData_Klinisk.csv ------------ Patients list and clinical data  (sheets \"Klinik fra Randi\")\n",
    "#                                        - (including medications, treatments etc...)\n",
    "# => ClinicalData_Oversikt.csv ----------- Sample list of all timepoint measurements (sheet \"Oversikt\")\n",
    "#                                        - (in theory 5 each patient)\n",
    "# => Lipoproteinene_Combined2.csv -------- Lipoprotein measurements (blood serum)\n",
    "# => Metabolittene_comb_final_clean.csv -- Metabolomics measurments (blood serum)\n",
    "\n",
    "path_Klinisk  = \"/Users/mattesa/molbreastlab-storage/work/Radiation_study/Matteo/ClinicalData_Klinisk.csv\" \n",
    "path_Oversikt = \"/Users/mattesa/molbreastlab-storage/work/Radiation_study/Matteo/ClinicalData_Oversikt.csv\" \n",
    "path_Lipopro  = \"/Users/mattesa/molbreastlab-storage/work/Radiation_study/Matteo/Lipoproteinene_Combined2.csv\" \n",
    "path_Metabol  = \"/Users/mattesa/molbreastlab-storage/work/Radiation_study/Matteo/Metabolittene_comb_final_clean.csv\" \n",
    "\n",
    "root_path = os.path.dirname(path_Lipopro)\n",
    "             \n",
    "# Load clinical data files, Lipoprotein and Metabolites measurments files \n",
    "# NOTE: There are some redundant columns!\n",
    "CD_klinisk = pd.read_csv( path_Klinisk,  header=1, index_col=False)\n",
    "CD_oversik = pd.read_csv( path_Oversikt, header=0, index_col=False)\n",
    "LP_measure = pd.read_csv( path_Lipopro,  header=0, index_col=False)\n",
    "MB_measure = pd.read_csv( path_Metabol,  header=0, index_col=False)\n",
    "\n",
    "\n",
    "# ------ PRUNING the data ----------------------------------------------------\n",
    "# Extract the timepoints as defined by Guro. We could also redifine by looking \n",
    "# dates or use dates to find matching measurments between different dataframes \n",
    "sele_oversik  = [ \"PatientID\", \n",
    "                  \"Samplenames lipo\", \"Names date modified\", \"Samling date\", \n",
    "                  \"Dato stråleterapi\", \n",
    "                  \"Timepoint\", \"Timepoint_kodet\",\n",
    "                  \"Alder\", \"BMI\", \n",
    "                  \"HEr2 \", \"ER \", \"PGR\",\"kjemo\", \"herceptin\" , \"hormornbeh\",\n",
    "                  \"FatPAS.1\", \"FatPAS.2\", \"FatPAS.3\", \"FatPAS.4\", \"FatPAS.5\"]\n",
    "CD_samples  = CD_oversik.loc[:,sele_oversik]\n",
    "\n",
    "# Simplify the names of the Metabolites dataframe columns, removing the unit\n",
    "names = MB_measure.columns\n",
    "MB_measure.columns = [xx.split(\" (RawConc)\")[0] for xx in names]\n",
    "\n",
    "# Rename column e.g. \"Samplenames lipo\" in CD_oversik === \"name\" in LP_measure\n",
    "CD_samples = CD_samples.rename(columns={\"Samplenames lipo\": \"Samplename\"})\n",
    "LP_measure = LP_measure.rename(columns={\"name\": \"Samplename\"})\n",
    "MB_measure = MB_measure.rename(columns={\"Sample Name\": \"Samplename\"})\n",
    "CD_samples = CD_samples.rename(columns={\"HEr2 \": \"HER2\"})\n",
    "CD_samples = CD_samples.rename(columns={\"ER \": \"ER\"})\n",
    "\n",
    "# Insert a \"PatientID\" column in LP and MB dataframes (redundant but good practice)\n",
    "pID = [ xx.split(\"_\")[0] for xx in LP_measure[\"Samplename\"]]\n",
    "LP_measure.insert( loc=1, column=\"PatientID\", value=pID)\n",
    "pID = [ xx.split(\"_\")[0] for xx in MB_measure[\"Samplename\"]]\n",
    "MB_measure.insert( loc=1, column=\"PatientID\", value=pID)\n",
    "\n",
    "\n",
    "# ! Ensure that the different DFs (CD, LP and MB) have the exact same set of \n",
    "#   samples in their list. Use \"Samplename\" compare and remove those samples\n",
    "#   that appear only in one dataframe.\n",
    "# NOTE: check manually on the list what we are actually removing !!!\n",
    "#       Specifically, LP have less samples than MP and CD\n",
    "# Use set-difference to find missing samples in LP_measure, then reomve those\n",
    "# from the list. Finally, subset DFs selecting only common samples\n",
    "miss_nn = np.setdiff1d( CD_samples[\"Samplename\"], LP_measure[\"Samplename\"])\n",
    "list_nn = np.setdiff1d( CD_samples[\"Samplename\"], miss_nn )\n",
    "CD_samples = CD_samples.loc[CD_samples[\"Samplename\"].isin(list_nn)]\n",
    "\n",
    "# As above, but between LP_measure and MB_measure\n",
    "# NOTE: MB_measure has repeated measurments for a couple of samples !!!\n",
    "miss_nn = np.setdiff1d( MB_measure[\"Samplename\"], LP_measure[\"Samplename\"])\n",
    "list_nn = np.setdiff1d( MB_measure[\"Samplename\"], miss_nn )\n",
    "MB_measure = MB_measure.loc[MB_measure[\"Samplename\"].isin(list_nn)]\n",
    "\n",
    "# Remove quality control measurments: strings starting with \"QC\"\n",
    "mask_QC = [\"QC\" not in xx   for xx in LP_measure[\"Samplename\"]]\n",
    "CD_samples = CD_samples.loc[ mask_QC, : ]\n",
    "LP_measure = LP_measure.loc[ mask_QC, : ]\n",
    "MB_measure = MB_measure.loc[ mask_QC, : ]\n",
    "\n",
    "\n",
    "# At this point we have the exact same entries in both DataFrames. \n",
    "# (Check by running np.setdiff1d in all combinations between the 3 DFs)\n",
    "# Sort-by name columns \"Samplename\" and then remove uncategorized timepoints.\n",
    "CD_samples = CD_samples.sort_values(\"Samplename\")\n",
    "LP_measure = LP_measure.sort_values(\"Samplename\")\n",
    "MB_measure = MB_measure.sort_values(\"Samplename\")\n",
    "\n",
    "\n",
    "# --- Selection by HER2 analysis\n",
    "# Correct Syntax errors in Time column ('pre ' (with space) instead of 'pre')\n",
    "idx = CD_samples.index[CD_samples['Timepoint']== 'pre '].tolist()\n",
    "CD_samples.loc[idx] = CD_samples.loc[idx]['Timepoint'] = 'pre'\n",
    "\n",
    "# Remove the samples tagged \"pre\" in HER2 column\n",
    "#mask = np.array(CD_samples[\"PGR\"] == -100)\n",
    "# Remove 'nan' values (CD_samples['Timepoint']== '?')\n",
    "mask = CD_samples[\"Timepoint_kodet\"].isnull().values\n",
    "CD_samples = CD_samples.loc[ ~mask, : ]\n",
    "LP_measure = LP_measure.loc[ ~mask, : ]\n",
    "MB_measure = MB_measure.loc[ ~mask, : ]\n",
    "\n",
    "\n",
    "\n",
    "# ------ REORGANIZE and SORT variables ---------------------------------------\n",
    "# Now all DFs the sample measurements sorted in the same order (Samplename)\n",
    "# Thus, we reindex the rows in all DFs to be the same.\n",
    "CD_samples = CD_samples.reset_index(drop=True)\n",
    "LP_measure = LP_measure.reset_index(drop=True)\n",
    "MB_measure = MB_measure.reset_index(drop=True)\n",
    "\n",
    "# Reorganize the \"covariates\" order so that we remove redundant (colinear) \n",
    "# factors and we can plot them together based on meaningful grouping\n",
    "# and order (e.g. class of molecules)\n",
    "# TG  = tryglyceride\n",
    "# CH  = total cholesterol\n",
    "# FC  = free cholesterol\n",
    "# (EC = esterified cholesterol) \n",
    "# PL  = phosppholipids\n",
    "# AB  = ApoB  (=== particle numbers, for specific class)\n",
    "# A1  = ApoA (only HDL)\n",
    "vars_LP_order = [  \n",
    "                'V1TG', 'V1CH', 'V1FC', 'V1PL', \n",
    "                'V2TG', 'V2CH', 'V2FC', 'V2PL', \n",
    "                'V3TG', 'V3CH', 'V3FC', 'V3PL', \n",
    "                'V4TG', 'V4CH', 'V4FC', 'V4PL', \n",
    "                'V5TG', 'V5CH', 'V5FC', 'V5PL', \n",
    "\n",
    "                'IDTG', 'IDCH', 'IDFC', 'IDPL',\n",
    "\n",
    "                'L1TG', 'L1CH', 'L1FC', 'L1PL',\n",
    "                'L2TG', 'L2CH', 'L2FC', 'L2PL',\n",
    "                'L3TG', 'L3CH', 'L3FC', 'L3PL',\n",
    "                'L4TG', 'L4CH', 'L4FC', 'L4PL',\n",
    "                'L5TG', 'L5CH', 'L5FC', 'L5PL',\n",
    "                'L6TG', 'L6CH', 'L6FC', 'L6PL',\n",
    "\n",
    "                'H1TG', 'H1CH', 'H1FC', 'H1PL', 'H1A1', 'H1A2',\n",
    "                'H2TG', 'H2CH', 'H2FC', 'H2PL', 'H2A1', 'H2A2',\n",
    "                'H3TG', 'H3CH', 'H3FC', 'H3PL', 'H3A1', 'H3A2',\n",
    "                'H4TG', 'H4CH', 'H4FC', 'H4PL', 'H4A1', 'H4A2',\n",
    "                ]\n",
    "vars_MB_order = [ 'Lysine', 'Histidine', 'Glutamic acid',\n",
    "                  'Glutamine', 'Asparagine', 'Threonine', \n",
    "                  'Glycine', 'Proline',\n",
    "                  'Phenylalanine', 'Leucine', 'Alanine', 'Tyrosine', 'Valine', 'Isoleucine', 'Methionine',                  \n",
    "                  'N,N-Dimethylglycine', 'Ornithine', 'Sarcosine',                 \n",
    "                  '2-Hydroxybutyric acid', '3-Hydroxybutyric acid', '2-Oxoglutaric acid', '2-Aminobutyric acid',\n",
    "                  'Pyruvic acid', 'Citric acid', 'Acetic acid', 'Lactic acid', 'Succinic acid', 'Formic acid', 'Acetoacetic acid',                  \n",
    "                  'K-EDTA','Ca-EDTA',                 \n",
    "                  'Acetone', 'Glycerol',                 \n",
    "                  'D-Galactose', 'Glucose',                 \n",
    "                  'Dimethylsulfone',                   \n",
    "                  'Choline', 'Creatine', 'Creatinine'\n",
    "                 ]\n",
    "\n",
    "data = LP_measure.iloc[:,4:]\n",
    "# Simplify the index (covariates) names by removing the \"unit\"\n",
    "names = data.columns.values\n",
    "names = [ xx.split(\" [\")[0] for xx in names]\n",
    "data.columns = names\n",
    "# select and sort on ly the variables in vars_LP_order\n",
    "data = data.loc[:,vars_LP_order] \n",
    "data = data.reindex(vars_LP_order, axis = 1)\n",
    "LP_measure = pd.concat( [LP_measure.iloc[:,0:4], data], axis = 1 )\n",
    "\n",
    "data = MB_measure.iloc[:,2:]\n",
    "data = data.reindex(vars_MB_order, axis = 1)\n",
    "MB_measure = pd.concat( [MB_measure.iloc[:,0:2], data], axis = 1 )\n",
    "\n",
    "\n",
    "\n",
    "# ------ SAVE ----------------------------------------------------------------\n",
    "# Save as Python-Ready .csv file\n",
    "CD_samples.to_csv( root_path + \"/PyR_Sample_ClinikData.csv\"  , header=True, index=True)\n",
    "LP_measure.to_csv( root_path + \"/PyR_Lipoproteine.csv\", header=True, index=True)\n",
    "MB_measure.to_csv( root_path + \"/PyR_Metabolittene.csv\", header=True, index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------ LOAD SURVIVAL data-tables -------------------------------------------\n",
    "# List of patients and importantly the survival. We need to load as .xlsx \n",
    "# because there are comment columns with \"commas\" that alter the table when \n",
    "# uploaded as .csv file\n",
    "# Survival state:  1= lever,  2= tilbakefall, 3= kreftdød,     4= annen død \n",
    "#                  1= alive,  2= relapse,     3= cancer death, 4= other death \n",
    "\n",
    "path_Survival = \"/Users/mattesa/molbreastlab-storage/work/Radiation_study/Matteo/Radiationstudy_survival_modified_MS.xlsx\" \n",
    "CD_survival = pd.read_excel( path_Survival,  header=0, index_col=False )\n",
    "\n",
    "# we do not take columns: \n",
    "# 6 - Komorbidity - a description and requires manual conversion to some categorical data type\n",
    "sele_survival = [ 0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 12]\n",
    "name_conversion = {\"PID\":             \"PatientID\",\n",
    "                   \"Dato start RT\":   \"Date_StartRadio\",\n",
    "                   \"10-års-kontroll lege/siste livstegn\": \"Date_10yCheck\",\n",
    "                   \"QOL-skjema\":           \"Date_QOLquestionnaire\",\n",
    "                   \"Kun QOL\":              \"With_QOL\",\n",
    "                   \"Antall dettatt\":       \"N_Participant\",\n",
    "                   \"Metastase/ ny kreft\":  \"NewCancer\",\n",
    "                   \"Dato mors\":            \"Date_Death\",\n",
    "                   \"Dato tilbakefall\":     \"Date_Relapse\",\n",
    "                   \"Oppfølgingstid\":       \"Followup\",\n",
    "                   \"Status, 1= lever, 2 = tilbakefall, 3 = kreftdød, 4 = annen død\":   \"Status\"\n",
    "                  }\n",
    "\n",
    "CD_survival = CD_survival.iloc[:, sele_survival]\n",
    "CD_survival = CD_survival.rename(columns= name_conversion)\n",
    "\n",
    "# Uniformize the date from strings to datetime64 \n",
    "# NOTE: NAN strings (possibly no follow up due to death) are convered into\n",
    "#       NaT in datetime64. To find NaT use .isnull() method\n",
    "CD_survival[\"Date_StartRadio\"] = pd.to_datetime(CD_survival[\"Date_StartRadio\"], infer_datetime_format=True, errors='coerce')\n",
    "CD_survival[\"Date_10yCheck\"]   = pd.to_datetime(CD_survival[\"Date_10yCheck\"], infer_datetime_format=True, errors='coerce')\n",
    "CD_survival[\"Date_Death\"]      = pd.to_datetime(CD_survival[\"Date_Death\"], infer_datetime_format=True, errors='coerce')\n",
    "CD_survival[\"Date_Relapse\"]    = pd.to_datetime(CD_survival[\"Date_Relapse\"], infer_datetime_format=True, errors='coerce')\n",
    "CD_survival[\"Date_QOLquestionnaire\"] = pd.to_datetime(CD_survival[\"Date_QOLquestionnaire\"], infer_datetime_format=True, errors='coerce')\n",
    "\n",
    "# Replace NaN and convert scolumns into simple 0-1 values\n",
    "mask    = CD_survival[\"N_Participant\"].isnull()\n",
    "CD_survival.loc[mask, \"N_Participant\"] = 0\n",
    "mask    = CD_survival[\"N_Participant\"].isnull()\n",
    "CD_survival.loc[mask, \"N_Participant\"] = 0\n",
    "mask    = CD_survival[\"NewCancer\"].isnull()\n",
    "CD_survival.loc[mask, \"NewCancer\"] = 0\n",
    "\n",
    "# Simplify survival state in binary: 0 deaths of any type; 1 for alive\n",
    "mask_1 = CD_survival[\"Status\"] == 1\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "mask_2 = CD_survival[\"Status\"] >= 2\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "\n",
    "CD_survival.loc[mask_1, \"Status_code\"] = 1\n",
    "CD_survival.loc[mask_2, \"Status_code\"] = 0\n",
    "#mask = CD_survival.loc[:, \"Status_code\"].isnull() \n",
    "\n",
    "\n",
    "# ------ SAVE ----------------------------------------------------------------\n",
    "# Save as Python-Ready .csv file\n",
    "CD_survival.to_csv( root_path + \"/PyR_Patients_Survival.csv\" , header=True, index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------ COMBINE ----------------------------------------------------------------\n",
    "# Add information on survival in the sample information DF: CD_samples\n",
    "# Initialize \"0\" column and then, for all alive PatientID replace with \"1\"\n",
    "CD_samples.insert( loc=8, column=\"Status_code\", value=0)\n",
    "mask = CD_survival.loc[:, \"Status_code\"] == 1\n",
    "pIDList = CD_survival.loc[mask, \"PatientID\"].values\n",
    "for ii_patient in pIDList: \n",
    "    CD_samples.loc[ CD_samples.loc[:,\"PatientID\"] == str(ii_patient) , \"Status_code\" ] = 1\n",
    "\n",
    "print(\"pID_Survive:  \", (CD_survival.loc[:, \"Status_code\"] == 1).sum() )\n",
    "print(\"pID_Dead   :  \", (CD_survival.loc[:, \"Status_code\"] == 0).sum() )\n",
    "print(\"Smp_Survive:  \", (CD_samples.loc[:, \"Status_code\"] == 1).sum() )\n",
    "print(\"Smp_Dead   :  \", (CD_samples.loc[:, \"Status_code\"] == 0).sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  -  115\n",
      "2  -  21\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as st\n",
    "import sys\n",
    "# Add as desired to the list of known working paths for Python \n",
    "sys.path.append('/Users/mattesa/ZenBook/Python/PLS-DA')\n",
    "\n",
    "import mbc_PLS_basic\n",
    "from mbc_PLS_basic import *\n",
    "# Force reload during development, otherwise Python only loads first time \n",
    "# and does not update unless the kernel is rebooted\n",
    "import importlib\n",
    "importlib.reload(mbc_PLS_basic)\n",
    "\n",
    "\n",
    "# --- 0 --- Parameters for the cross validation ------------------------------\n",
    "max_N_comp = 10           # Maximum number of components to tes\n",
    "outer_PropSele = 0.20     # proportion train-to-test to use\n",
    "inner_PropSele = 0.20    \n",
    "outer_loop  = 40           # cycles number to perform double cross val.\n",
    "inner_loop  = 10\n",
    "min_cat = 1\n",
    "max_cat = 2\n",
    "\n",
    "# Transform categories into 0 and 1 response values\n",
    "transform_cat = True\n",
    "if transform_cat == True :\n",
    "    min_rval = 0\n",
    "    max_rval = 1\n",
    "    p_threshold = 0.5                       # predicted probability threshold\n",
    "else:\n",
    "    min_rval = min_cat\n",
    "    max_rval = max_cat\n",
    "    p_threshold = (min_cat+max_cat) / 2     # predicted probability threshold\n",
    "\n",
    "\n",
    "# Column name for the response variable to use in PLS-DA\n",
    "fold_save            = \"Survival\"\n",
    "unique_PatientID_Col = \"PatientID\"\n",
    "Response_Col         = \"ER\"\n",
    "therapy_type         = \"None\"\n",
    "therapy_group        = 0\n",
    "data_type            = \"LP\"\n",
    "\n",
    "             \n",
    "\n",
    "# --- 1 --- Prepare the DataFrames in the appropriate format -----------------\n",
    "# Prepare the data we wish to use for the PLS model in XX and YY dataframes.\n",
    "YY = CD_samples.copy()\n",
    "# Chose the dataset according to the data_type\n",
    "if   data_type == \"LP\":\n",
    "    dataset = LP_measure               \n",
    "elif data_type == \"MB\":\n",
    "    dataset = MB_measure \n",
    "elif data_type == \"LP-MB\":\n",
    "    dataset = pd.concat([LP_measure, MB_measure.iloc[:,2:]], axis = 1)\n",
    "    \n",
    "# Select only samples from a specific time point\n",
    "mask = YY[\"Timepoint_kodet\"]== 5\n",
    "YY = YY.loc[mask]\n",
    "dataset = dataset.loc[mask]\n",
    "\n",
    "# Remove samples with NaN values in Response_Col\n",
    "mask = YY[Response_Col].isnull().values\n",
    "YY = YY.loc[ ~mask, : ]\n",
    "dataset = dataset.loc[ ~mask, : ]\n",
    "\n",
    "# SAMPLE SELECTION based on Response_Col, time, surviva, fatigue...\n",
    "# YY.loc[ YY[Response_Col]==0, Response_Col] = 0\n",
    "# YY.loc[ YY[Response_Col]>=1, Response_Col] = 1\n",
    "# YY.loc[ YY[therapy_type]>=1, therapy_type] = 1\n",
    "# YY = YY.loc[ YY[therapy_type]==therapy_group, :] \n",
    "\n",
    "# Display how many sample there are for each category.\n",
    "col_name = \"Status_code\"\n",
    "data_column = YY[Response_Col]\n",
    "categ = [min_cat, max_cat]\n",
    "for cc in categ:\n",
    "    tot = ( data_column == cc).sum()\n",
    "    print(cc, \" - \",  tot)\n",
    "\n",
    "# Scale-standardize the XX matrix and reduce selection only for data with \n",
    "# specific values in \"Response_Col\" column (min_cat, max_cat)\n",
    "XX_vars_scaled, XX_scaled = StandardScale_FeatureTable( dataset , 4)\n",
    "XX, YY = CrossSelect_TwoTables( XX_vars_scaled, YY, Response_Col, [min_cat,max_cat], transform_cat ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:   : 25 of 40\r"
     ]
    }
   ],
   "source": [
    "# --- 2 --- Cross-validation of PLS ------------------------------------------\n",
    "# Outer\n",
    "#   Inner\n",
    "#     number LV\n",
    "PerfoMetric, comparPred, outerCAL, totalCAL, optimal_nLV = optimise_PLS_CrossVal(\n",
    "                        XX, YY, max_N_comp, unique_PatientID_Col,\n",
    "                        Response_Col, min_rval, max_rval, p_threshold , \n",
    "                        outer_loop, inner_loop, outer_PropSele, inner_PropSele,\n",
    "                        True)\n",
    "# Calculate the Accuracy CI (confidence intervals)\n",
    "data  = PerfoMetric.loc[\"Accuracy\"].values.tolist()\n",
    "CI = st.t.interval(0.95, len(data)-1, loc=np.mean(data), scale=st.sem(data))\n",
    "CI = [ CI[xx] for xx in range(len(CI))]      # tuple-to-list conversion\n",
    "\n",
    "# Visuallize key results on accuracy and optimal LV number\n",
    "print(\"\\n\")\n",
    "print(\"Mode num. LV:  \" + str(optimal_nLV) )\n",
    "PerfoMetric[\"Mean\"] = PerfoMetric.mean(axis=1)\n",
    "print(\"Accuracy    :  \" + str(round( PerfoMetric.mean(axis=1)[\"Accuracy\"], 2))\n",
    "                        + \"  ( \" +str(round(CI[0],4))+\", \"+str(round(CI[1],4))+ \" )\" ) \n",
    "\n",
    "pd.DataFrame( PerfoMetric[\"Mean\"] )\n",
    "\n",
    "# plot_metrics(outerMSE.iloc[ 0:1, :].T.values, 'MSE', 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics( outerCAL.T.values , 'ACCU', 'min', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (136, 3), indices imply (136, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/opt/miniconda/envs/myBase/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1653\u001b[0m                 blocks = [\n\u001b[0;32m-> 1654\u001b[0;31m                     \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1655\u001b[0m                 ]\n",
      "\u001b[0;32m~/opt/miniconda/envs/myBase/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[1;32m   3040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3041\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda/envs/myBase/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m    124\u001b[0m             raise ValueError(\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0;34mf\"Wrong number of items passed {len(self.values)}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                 \u001b[0;34mf\"placement implies {len(self.mgr_locs)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 3, placement implies 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-632-6caae7cdefaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mpls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m df = pd.DataFrame(np.column_stack([pls.x_scores_, opls.T_ortho_[:, 0]]),\n\u001b[0;32m---> 77\u001b[0;31m                   index=M_X.index, columns=['t', 't_ortho'])                           \n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mpos_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM_Y\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mneg_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM_Y\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda/envs/myBase/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda/envs/myBase/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[0;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mblock_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda/envs/myBase/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1662\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"values\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1664\u001b[0;31m         \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda/envs/myBase/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   1692\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mblock_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (136, 3), indices imply (136, 2)"
     ]
    }
   ],
   "source": [
    "# ************************************************************\n",
    "#                          O-PLS\n",
    "# ************************************************************\n",
    "\n",
    "from pyopls import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import cross_val_predict, LeaveOneOut, KFold\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "\n",
    "M_X = XX\n",
    "M_Y = YY[Response_Col] # response\n",
    "Resp_class1 = 0\n",
    "Resp_class2 = 1\n",
    "P_Threshold = 0.5\n",
    "EvalResults = pd.DataFrame( 0 , index = [\"Best_nLV\", \"Accuracy\", \"Specificity\", \"Sensitivity\"],\n",
    "                                columns = [ \"o\"+str(xx+1) for xx in range(5)] )\n",
    "ooL = 0   \n",
    "\n",
    "    \n",
    "cv_split = KFold(n_splits=8, shuffle=False, random_state=None)\n",
    "\n",
    "# Build PLS predicion model with N number of LV (for comparison to oPLS) \n",
    "pls = PLSRegression(2)\n",
    "\n",
    "# Create the class OPLS, with the number of orthogonal components to filter\n",
    "opls = OPLS(4)                # opt_nLV\n",
    "# In sequence, fit the model to the data, then get the non-orthogonal \n",
    "# components of X (which are considered in prediction)\n",
    "# I.e. Learn and apply filtering on the training data and get the filtered X, \n",
    "# called variable Z in the code\n",
    "Z = opls.fit_transform(M_X, M_Y)\n",
    "# Now that data is orthogonalized, we can Build PLS predicion model \n",
    "processed_y_pred     = cross_val_predict(pls, Z, M_Y, cv = cv_split)\n",
    "processed_q_squared  = r2_score(M_Y, processed_y_pred)                   # 0.981\n",
    "processed_dq_squared = r2_score(M_Y, np.clip(processed_y_pred, -1, 1))   # 0.984\n",
    "processed_accuracy   = accuracy_score(M_Y, np.sign(processed_y_pred))    # 1.0\n",
    "\n",
    "\n",
    "dff, Accu_Spe_Sele = binary_classification( M_Y.values, processed_y_pred.flatten(), 0.5 , 0, 1 )\n",
    "\n",
    "\n",
    "EvalResults.iloc[:, ooL] = Accu_Spe_Sele\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "comparPred = pd.DataFrame( [M_Y.tolist(), processed_y_pred, Y_pred_thres ] ).T\n",
    "comparPred.columns = [\"M_Y\", \"oY_pred\", \"Y_pred_thres\"]\n",
    "\n",
    "# ? ? ?\n",
    "r2_X = opls.score(M_X)  # 7.8e-12 (most variance is removed)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(M_Y, y_pred)\n",
    "roc_auc = roc_auc_score(M_Y, y_pred)\n",
    "proc_fpr, proc_tpr, proc_thresholds = roc_curve(M_Y, processed_y_pred)\n",
    "proc_roc_auc = roc_auc_score(M_Y, processed_y_pred)\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(fpr, tpr, lw=2, color='blue', label=f'Unprocessed (AUC={roc_auc:.4f})')\n",
    "plt.plot(proc_fpr, proc_tpr, lw=2, color='red',\n",
    "         label=f'39-component OPLS (AUC={proc_roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "pls.fit(Z, M_Y)\n",
    "df = pd.DataFrame(np.column_stack([pls.x_scores_, opls.T_ortho_[:, 0]]),\n",
    "                  index=M_X.index, columns=['t', 't_ortho'])                           \n",
    "pos_df = df[M_Y==1]\n",
    "neg_df = df[M_Y==0]\n",
    "\n",
    "plt.figure(1)\n",
    "plt.scatter(pos_df['t'], pos_df['t_ortho'], c='blue',  label=1 )   # 'Cancer'\n",
    "plt.scatter(neg_df['t'], neg_df['t_ortho'], c='red',   label=0 )   # 'Control'\n",
    "plt.title('PLS Scores')\n",
    "plt.xlabel('t_ortho')\n",
    "plt.ylabel('t')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230, 12)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opls.T_ortho_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230, 1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pls.x_scores_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 12)"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opls.P_ortho_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pls.x_loadings_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mbc_PLS_basic\n",
    "from mbc_PLS_basic import *\n",
    "# Force reload during development, otherwise Python only loads first time \n",
    "# and does not update unless the kernel is rebooted\n",
    "import importlib\n",
    "importlib.reload(mbc_PLS_basic)\n",
    "\n",
    "\n",
    "def RandomSelect_P_TrainTest_2( M_X, M_Y, uID_colname, proportion, RespVar, RespClasses):\n",
    "    '''\n",
    "    # Create training and test sets for M_X and M_Y. Moreover, the samples \n",
    "    # subset are created to ensure that all classes (RespClasses) in response \n",
    "    # variable (RespVar) are selected with the choosen proportion. If we do \n",
    "    # not account this when one class is less numerous it can be that the \n",
    "    # random selection results in disproportionate representation, skewing \n",
    "    # and compromising the calculation of PLS performance accuracy.   \n",
    "    #\n",
    "    # (NOTE : The two matrices are assumed to have same order)\n",
    "    # uID_colname = column name with the unique ID, it is assumed to already \n",
    "    #               be a unique list (i.e. only one \"patient\" is present in \n",
    "    #               each matrix)\n",
    "    '''\n",
    "    import random    \n",
    "    # Initialize indexes storing variables\n",
    "    test_pID  = [];        train_pID = [];   \n",
    "     \n",
    "    for ii in range(len(RespClasses)) : \n",
    "        # Create subsets of pID that all have same class as response variable \n",
    "        sub_M_Y = M_Y.loc[ M_Y[RespVar] == RespClasses[ii] , :]\n",
    "        # Generate unique list of identifiers to use to create the two sets\n",
    "        uID_List  = np.unique( sub_M_Y[ uID_colname ].values )\n",
    "        random.shuffle(uID_List)\n",
    "        # Collect and store the IDs selected for this subset\n",
    "        test_pID.extend(  random.sample( uID_List.tolist(), round(len(uID_List)*proportion)) )\n",
    "        train_pID.extend( np.setdiff1d(uID_List, test_pID) )\n",
    "        \n",
    "    # Use \"pID\" to create Train/Test dataset using the the full matrices   \n",
    "    mask_train = M_Y[uID_colname].isin( train_pID )\n",
    "    mask_test  = M_Y[uID_colname].isin( test_pID ) \n",
    "    X_train = M_X.loc[ mask_train , :] \n",
    "    X_test  = M_X.loc[ mask_test  , :]\n",
    "    Y_train = M_Y.loc[ mask_train , :] \n",
    "    Y_test  = M_Y.loc[ mask_test  , :]   \n",
    "    \n",
    "    # Quick-check to display of the results count and proportion\n",
    "    N_train = X_train.shape[0]\n",
    "    N_test  = X_test.shape[0]\n",
    "\n",
    "    n_train_c1 = sum(Y_train[RespVar] == 0)\n",
    "    n_train_c2 = sum(Y_train[RespVar] == 1)\n",
    "    n_test_c1 =  sum(Y_test[RespVar] == 0)\n",
    "    n_test_c2 =  sum(Y_test[RespVar] == 1)\n",
    "    print(\"Train N:  \", N_train )\n",
    "    print(\"Test N:   \", N_test )\n",
    "    print(\"               |Class 1 | Class 2\" )\n",
    "    print(\"Train tot N    | \", n_train_c1, \"   |  \", n_train_c2)\n",
    "    print(\"Test  tot N    | \", n_test_c1,  \"   |  \", n_test_c2)\n",
    "    print(\"Train rel Prop | \", round(n_train_c1/N_train, 2), \" |  \", round(n_train_c2/N_train, 2) )\n",
    "    print(\"Test  rel Prop | \", round(n_test_c1/N_test, 2),   \" |  \", round(n_test_c2/N_test, 2) )\n",
    "\n",
    "    return X_train, X_test ,Y_train, Y_test\n",
    "\n",
    "\n",
    "\n",
    "unique_PatientID_Col = \"PatientID\"\n",
    "Response_Col         = \"ER\"\n",
    "\n",
    "X_train, X_test ,Y_train, Y_test = RandomSelect_P_TrainTest( XX, YY, unique_PatientID_Col, 0.2, Response_Col, [0,1]  )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4       True\n",
       "7       True\n",
       "14     False\n",
       "16      True\n",
       "21      True\n",
       "       ...  \n",
       "972     True\n",
       "975     True\n",
       "978     True\n",
       "981     True\n",
       "987     True\n",
       "Name: ER, Length: 136, dtype: bool"
      ]
     },
     "execution_count": 961,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YY[Response_Col] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
